{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face <img src=\"figs/huggingface_logo-noborder.svg\" width=\"50\" title=\"https://huggingface.co/\"/>\n",
    "\n",
    "https://huggingface.co/course/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?\n",
    "\n",
    "NLP is a field of linguistics and machine learning focused on understanding everything related to human language. The aim of NLP tasks is not only to understand single words individually, but to be able to understand the context of those words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of NLP tasks:\n",
    "\n",
    "- Classifying whole sentences\n",
    "\n",
    "- Classifying each word in a sentence\n",
    "\n",
    "- Generating text content\n",
    "\n",
    "- Extracting an answer from a text\n",
    "\n",
    "- Generating a new sentence from an input text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline function (pipeline())\n",
    "\n",
    "It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main pipelines steps:\n",
    "\n",
    "1. The text is preprocessed into a format the model can understand.\n",
    "\n",
    "2. The preprocessed inputs are passed to the model.\n",
    "\n",
    "3. The predictions of the model are post-processed, so you can make sense of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available pipelines\n",
    "\n",
    "- feature-extraction (get the vector representation of a text)\n",
    "\n",
    "- fill-mask\n",
    "\n",
    "- ner (named entity recognition)\n",
    "\n",
    "- question-answering\n",
    "\n",
    "- sentiment-analysis\n",
    "\n",
    "- summarization\n",
    "\n",
    "- text-generation\n",
    "\n",
    "- translation\n",
    "\n",
    "- zero-shot-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot classification\n",
    "\n",
    "The zero-shot-classification pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don’t have to rely on the labels of the pretrained model.\n",
    "This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "\n",
    "The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using any model from the Hub in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask filling\n",
    "\n",
    "Fill in the blanks in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition (NER)\n",
    "\n",
    "Is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (ORG), and Brooklyn a location (LOC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering\n",
    "\n",
    "Pipeline answers questions using information from a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "Is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation\n",
    "\n",
    "You can use a default model if you provide a language pair in the task name, but the easiest way is to pick the model you want to use on the Model Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import TFAutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-hy-en')\n",
    "translator = pipeline(\"translation\", model=model)\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Archtecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer architecture was introduced in June 2017. The focus of the original research was on translation tasks.\n",
    "\n",
    "- GPT (June 2018)\n",
    "\n",
    "- BERT (October 2018)\n",
    "\n",
    "- GPT-2 (February 2019)\n",
    "\n",
    "- BART/T5 (October 2019)\n",
    "\n",
    "- GPT-3 (May 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly, they can be grouped into three categories:\n",
    "\n",
    "1. GPT-like (also called auto-regressive Transformer models)\n",
    "\n",
    "2. BERT-like (also called auto-encoding Transformer models)\n",
    "\n",
    "3. BART/T5-like (also called sequence-to-sequence Transformer models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers are language models\n",
    "\n",
    "- All the Transformer models mentioned above have been trained as language models. \n",
    "\n",
    "- This means they have been trained on large amounts of raw text in a self-supervised fashion.\n",
    "\n",
    "- Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers are big models\n",
    "\n",
    "- The general strategy to achieve better performance is by increasing the models’ sizes as well as the amount of data they are pretrained on.\n",
    "\n",
    "- Training a model, especially a large one, requires a large amount of data. \n",
    "\n",
    "- This becomes very costly in terms of time and compute resources.\n",
    "\n",
    "<img src=\"figs/carbon_footprint-dark.svg\" width=\"1000\" title=\"https://huggingface.co/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Pretraining***\n",
    "\n",
    "- *Pretraining* is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.\n",
    "\n",
    "- *Pretraining* is usually done on very large amounts of data. \n",
    "\n",
    "- It requires a very large corpus of data, and training can take up to several weeks.\n",
    "\n",
    "<img src=\"figs/pretraining-dark.svg\" width=\"700\" title=\"https://huggingface.co/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Fine-tuning***\n",
    "\n",
    "- *Fine-tuning* is the training done after a model has been pretrained.\n",
    "\n",
    "- To perform *fine-tuning*, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task.\n",
    "\n",
    "- The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset.\n",
    "\n",
    "- The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining.\n",
    "\n",
    "- Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.\n",
    "\n",
    "- The amount of time and resources needed to get good results are much lower.\n",
    "\n",
    "<img src=\"figs/finetuning-dark.svg\" width=\"700\" title=\"https://huggingface.co/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformes Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "- Receives an input and builds a representation of it (its features).\n",
    "\n",
    "- The model is optimized to acquire understanding from the input.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "- Uses the encoder’s representation (features) along with other inputs to generate a target sequence.\n",
    "\n",
    "- The model is optimized for generating outputs.\n",
    "\n",
    "<img src=\"figs/transformers_blocks-dark.svg\" width=\"700\" title=\"https://huggingface.co/\"/>\n",
    "\n",
    "- Each of these parts can be used independently, depending on the task:\n",
    "\n",
    "1. **Encoder-only models**: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
    "\n",
    "2. **Decoder-only models**: Good for generative tasks such as text generation.\n",
    "\n",
    "3. **Encoder-decoder models** or **sequence-to-sequence models**: Good for generative tasks that require an input, such as translation or summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Fine-Tuning a Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('transformers-course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5997f4302721c0f127d60e8eacbcc67e6105138c3047541c99f4d9789cb1e96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
