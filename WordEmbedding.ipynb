{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "## Universidade Federal de Lavras\n",
    "\n",
    "### Agosto de 2022\n",
    "\n",
    "Elaborado por: <a href=\"https://github.com/Victorgonl\">Victor Gonçalves Lima</a>\n",
    "\n",
    "Orientado por: <a href=\"https://sites.google.com/ufla.br/denilsonpereira/\">Prof. Denilson Alves Pereira</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento de Linguagens Naturais (NLP)\n",
    "\n",
    "- Subcampo de línguistica, ciências da computação e inteligência artificial que estuda como computadores processam e analizam dados de **linguagem natural** humana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "- Termo que se refere à representação de palavras para análises textuais, de forma que vetores usados para codificar certa palavra seja espacialmente próximo aos vetores de palavras similares.\n",
    "\n",
    "- Podem ser obtidos através de **modelos de linguagens**, onde palavras e sentenças são mapeadas em vetores de números reais.\n",
    "\n",
    "<img src=\"figs/WordEmbedding.png\" width=\"600\" title=\"https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "- Neural network com vantagens comparado com **RNN**, **LSTM** e **GRU**.\n",
    "\n",
    "- Possui modelos mais efetivos para lidar com dependências em uma sequência temporal.\n",
    "\n",
    "- Possui um treinamento mais eficiente, eliminando dependências sequenciais de tokens anteriores.\n",
    "\n",
    "- Possui uma arquitetura **encoder-decoder**, usando **mecanismos de atenção**.\n",
    "\n",
    "- Passa a sequência para o decoder de uma vez, ao envés de sequencialmente.\n",
    "\n",
    "<img src=\"figs/Transformer-20fps.gif\" width=\"500\" title=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abordagens para representação de palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Lookup\n",
    "\n",
    "- Método que atribui uma identificação para cada palavra.\n",
    "\n",
    "- Pode utilizar **lemmatisation** e **stemming** para converter palavras para sua forma básica, sem conjugações por exemplo.\n",
    "\n",
    "- Um valor que não pertence ao vocabulário é chamado **out of vocabulary**.\n",
    "\n",
    "- **Out of vocabulary** é representado pelo número do tamanho *n* do vocabulário.\n",
    "\n",
    "- Por trabalhar com números inteiros, os modelos podem, de forma errônea, assumir a existência de ordem pelos IDs.\n",
    "\n",
    "<img src=\"figs/DicionaryLookup.png\" width=\"700\" title=\"https://towardsdatascience.com/word-representation-in-natural-language-processing-part-i-e4cd54fed3d4\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks\n",
      "walking\n",
      "sleeps\n",
      "learning\n",
      "cars\n",
      "learned\n"
     ]
    }
   ],
   "source": [
    "path_vocab = \"./datasets/words.txt\"\n",
    "\n",
    "words = []\n",
    "n_words = 0\n",
    "\n",
    "with Path(path_vocab).open() as f: \n",
    "    for word in f: \n",
    "        words.append(word.strip())\n",
    "        print(word.strip(), sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn\n",
      "rock\n",
      "car\n",
      "sleep\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "base_words = set()\n",
    "\n",
    "for word in words:\n",
    "    word = WordNetLemmatizer().lemmatize(word)\n",
    "    word = PorterStemmer().stem(word)\n",
    "    base_words.add(word)\n",
    "\n",
    "for word in base_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\t\tWord\n",
      "-------\t\t-------\n",
      "0\t\tlearn\n",
      "1\t\trock\n",
      "2\t\tcar\n",
      "3\t\tsleep\n",
      "4\t\twalk\n"
     ]
    }
   ],
   "source": [
    "base_words = list(base_words)\n",
    "\n",
    "print(\"ID\\t\\tWord\")\n",
    "print(\"-------\\t\\t-------\")\n",
    "for i in range(len(base_words)):\n",
    "    print(i, \"\\t\\t\", base_words[i], sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Enconding\n",
    "\n",
    "- É uma representação vetorial onde os vetores possuem o tamanho *n* do vocabulário + 1 (*n + 1*).\n",
    "\n",
    "- O vetor de uma palavra é preenchido de 0s, exceto pela posição que representa a palavra, preenchida por 1.\n",
    "\n",
    "- A posição *n* representa o valor **out of vocabulary**.\n",
    "\n",
    "- Possui vantagem sobre **Dictionary Lookup**, porém requer muita memória para computação devido ao tamanho.\n",
    "\n",
    "<img src=\"figs/One-HotEncoding.png\" width=\"900\" title=\"https://towardsdatascience.com/word-representation-in-natural-language-processing-part-i-e4cd54fed3d4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criada a partir do conceito de que palavras que aparecem em contextos similares possuem significados similares.\n",
    "\n",
    "- Apresenta os contextos que uma palavra occore em matriz.\n",
    "\n",
    "- As linhas da tabela representa as palavras do vocabulário e as colunas são seus contextos.\n",
    "\n",
    "- As entradas na tabela são o número de ocorrências de uma palavra em determinado contexto.\n",
    "\n",
    "<img src=\"figs/DistributionalRepresentation.png\" width=\"750\" title=\"https://towardsdatascience.com/word-representation-in-natural-language-processing-part-i-e4cd54fed3d4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Representa palavras como vetores.\n",
    "\n",
    "- Cada entrada no vetor representa uma ***hiden feature*** no significado da palavra.\n",
    "\n",
    "- Podem revelar dependências semânticas e sintáticas.\n",
    "\n",
    "É possível perceber na representação abaixo que 'flight' e 'plane' possuem valores numéricos próximos, assim como 'river' e 'lake'.\n",
    "\n",
    "<img src=\"figs/DistributionalRepresentation-2.png\" width=\"1000\" title=\"https://towardsdatascience.com/word-representation-in-natural-language-processing-part-i-e4cd54fed3d4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (Skip-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modelo **Skip-Gram** faz parte da biblioteca **Word2Vec**.\n",
    "\n",
    "- A ideia principal é representar palavras através de seus vizinhos.\n",
    "\n",
    "- Tenta prever o contexto de uma determinada palavra.\n",
    "\n",
    "- É possível treinar o **Word2Vec** com diferentes **datasets**, ou carregar dados pré-treinados, como disponibilizado pela Google.\n",
    "\n",
    "<img src=\"figs/Skip-Gram-Arch.png\" width=\"300\" title=\"https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\"/>\n",
    "\n",
    "<h1>\n",
    "<img src=\"figs/Skip-Gram-Model1.png\" width=\"300\" title=\"https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\"/>\n",
    "<img src=\"figs/Skip-Gram-Model2.png\" width=\"272\" title=\"https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\"/>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe (Global Vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Diferente do ***Word2Vec***, que captura certos contextos locais, ***GloVe*** explora sobre toda estatística de ocorrência da palavra no dataset.\n",
    "\n",
    "<img src=\"figs/GloVe-Visualization.jpg\" width=\"300\" title=\"https://nlp.stanford.edu/projects/glove/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Etapas:\n",
    "\n",
    "1. Criar uma matriz de coocorrências de termos.\n",
    "\n",
    "2. Para cada palavra, é computado as probabilidades condicionais $P(k|w)$.\n",
    "\n",
    "3. $w$ é a palavra sendo analisada e $k$ é uma outra palavra do vocabulário.\n",
    "\n",
    "4. Quanto maior o valor de $P$, maior a probabilidade da palavra $w$ estar correlacionada com a palavra $k$.\n",
    "\n",
    "5. Após todas as computações estatísticas, a matriz é formada.\n",
    "\n",
    "6. A matriz é reduzida por ***normalização*** e ***exponential smoothing***.\n",
    "\n",
    "<img src=\"figs/GloVe-Example.png\" width=\"800\" title=\"https://nlp.stanford.edu/projects/glove/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas com **Word2Vec** e **GloVe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Possuem um único **embedding** para cada palavra.\n",
    "\n",
    "- Algumas palavras podem ter mais de um significado, completamente distintos.\n",
    "\n",
    "- Os modelos, assim, não conseguem atribuir múltiplos significados para uma mesma palavra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ELMo** (**E**mbeddings from **L**anguage **Mo**dels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura\n",
    "\n",
    "É dividido em três camadas:\n",
    "\n",
    "**1. CNN:** A entrada do modelo é puramente baseada em caracteres, que passarão por CNN produzindo **compact embedding** a ser passados para **Bi-LSTM**.\n",
    "\n",
    "**2. Bi-LSTM:** A camada **Bi-LSTM** permite que o modelo execute tanto em **time steps** anteriores quanto posteriores. Cada LSTM pega entradas anteriores, exceto a primeira que recebe da primeira camada **CNN**.\n",
    "\n",
    "**3. Embedding:** A camada de **embedding** concatena os **hidden states** da **LSTM** produzindo **context-dependent embeddings**.\n",
    "\n",
    "<img src=\"figs/ELMo-Arch.png\" width=\"400\" title=\"https://towardsdatascience.com/word-representation-in-natural-language-processing-part-iii-2e69346007f\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT** (**B**idirectional **E**ncoder **R**epresentations from **T**ransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O modelo é baseado em **encoders** de **Transformer** de forma bidirecional.\n",
    "\n",
    "- Apresenta um modelo de linguagem para uso geral que suporta **transfer learning** e **fine tuning** com datasets específicos.\n",
    "\n",
    "<img src=\"figs/BERT-Arch1.png\" width=\"700\" title=\"http://jalammar.github.io/illustrated-bert/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Foi desenvolvido dois modelos principais:\n",
    "\n",
    "1. **BASE**: Number of transformer blocks (L): 12, Hidden layer size (H): 768 and Attention heads(A): 12\n",
    "\n",
    "2. **LARGE**: Number of transformer blocks (L): 24, Hidden layer size (H): 1024 and Attention heads(A): 16\n",
    "\n",
    "<img src=\"figs/BERT-BASE&LARGE.png\" width=\"700\" title=\"http://jalammar.github.io/illustrated-bert/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokens usados para **fine tuning**:\n",
    "\n",
    "1. **CLS**: o primeiro token de cada sequência, usado normalmente em conjunção com uma camada *softmax* para problemas de classificação.\n",
    "\n",
    "2. **SEP**: token delimitador de sequência usado para separar problemas que utilizam um par de sequência.\n",
    "\n",
    "3. **MASK**: token usado para **masked words**.\n",
    "\n",
    "<img src=\"figs/BERT-Arch3.png\" width=\"900\" title=\"https://arxiv.org/pdf/1810.04805.pdf\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Token embeddings** são as identificações do vocabulário para cada um dos tokens.\n",
    "\n",
    "- **Sentence Embeddings** é uma classe numérica para distinguir duas sentanças.\n",
    "\n",
    "- **Transformer positional embeddings** indica a posição de cada palavra na sequência.\n",
    "\n",
    "<img src=\"figs/BERT-Arch2.png\" width=\"900\" title=\"https://arxiv.org/pdf/1810.04805.pdf\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cada saída por token de cada camada de **encoder de transformer** pode ser usado para **word embedding**.\n",
    "\n",
    "- Os autores idetificaram que o melhor a ser usado como **embedding** seria a concatenação das últimas quatro camadas.\n",
    "\n",
    "<img src=\"figs/BERT-Embedding.png\" width=\"900\" title=\"https://arxiv.org/pdf/1810.04805.pdf\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "\n",
    "- https://towardsdatascience.com/word-representation-in-natural-language-processing-part-i-e4cd54fed3d4\n",
    "\n",
    "- https://towardsdatascience.com/word-representation-in-natural-language-processing-part-ii-1aee2094e08a\n",
    "\n",
    "- https://towardsdatascience.com/word-representation-in-natural-language-processing-part-iii-2e69346007f\n",
    "\n",
    "- https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b\n",
    "\n",
    "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "- https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/\n",
    "\n",
    "- https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
    "\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "- https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n",
    "\n",
    "- https://arxiv.org/pdf/1810.04805.pdf\n",
    "\n",
    "- http://jalammar.github.io/illustrated-bert/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
